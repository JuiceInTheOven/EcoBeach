services:
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      cscourse:
        ipv4_address: 172.202.0.9
  spark-worker:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    command: > # The NDWIAnalyzer Spark Job requires a few dependencies in the system to allow us to use Pillow (python package) on the Workers.
      bash -c "apk add zlib-dev jpeg-dev gcc musl-dev python3-dev
      && pip3 install Pillow
      && bash /worker.sh"  
    networks: 
      cscourse:
        ipv4_address: 172.202.0.10

networks:
  cscourse:
    name: hadoop
    ipam:
      config:
        - subnet: 172.202.0.0/24